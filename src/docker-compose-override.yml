services:
  llm-agent:
    profiles: ["gui"]
    platform: linux/amd64
    container_name: llm-agent
    build: .
    depends_on:
      - omeroserver
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LLM_BACKEND: ${LLM_BACKEND:-huggingface}
      HF_MODEL: ${HF_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
      HF_TOKEN: ${HF_TOKEN}
      GEMINI_MODEL: ${GEMINI_MODEL:-gemini-1.5-flash-latest}
      GEMINI_API_KEY: ${GEMINI_API_KEY}
      OMERO_HOST: omeroserver
      OMERO_PORT: "4064"
      DISPLAY: ":99"
    
    networks:
      - omero
    
    ports:
      - "6080:6080"
      - "5901:5901"
    
    volumes:
      - ./llm-agent:/app        
      - vae_cache:/app/.cache
    
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

volumes:
  vae_cache: {}
